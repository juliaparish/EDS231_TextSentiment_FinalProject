---
title: "Final Project"
author: "Paloma Cartwright, Allie Cole, Wylie Hampson, Ben Moscona-Remnitz, Julia Parish"
date: '2022-06-01'
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
output: 
  pdf_document:
    extra_dependencies: ["float"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.pos='!H')
```

# Sentiment of Sustainable Agriculture over 30 year period

This text sentiment analysis was completed as an assignment for the course, Environmental Data Science 231: Text and Sentiment Analysis for Environmental Problems. The data was sourced from  ...

Original assignment instructions can be found [here](https://maro406.github.io/EDS_231-text-sentiment/Group_Project.html)

### Load Libraries

```{r packages}
#install packages as necessary, then load libraries
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}

librarian::shelf(here,
                 igraph,
                 kableExtra,
                 ldatuning,
                 LDAvis,
                 LexisNexisTools,
                 lubridate,
                 pdftools,
                 quanteda,
                 quanteda.textplots,
                 quanteda.textstats,
                 readr,
                 reshape2,
                 sentimentr,
                 tidyr,
                 tidytext,
                 tidyverse,
                 tm,
                 topicmodels,
                 tsne, 
                 janitor)
```


# Nexis Uni Data - Sustainable Agriculture Sentiment past 30 years

Data consists of news articles, law reviews, and journals using the Nexis Uni database, and published between the years 1990-2021 using the search terms “Integrated Pest Management” and “sustainable farming.” Due to Nexis Uni limiting article returns to 100 documents per search, we collected the 100 most relevant articles from each year. When a year returned less than 100 articles for the search terms, all returned articles were collected. Articles were not geographically restricted, but they were all published in English. 

```{r}
nu_folder <- here("data/text_data")

nu_data <- list.files(pattern = ".DOCX", path = nu_folder, 
                        full.names = TRUE, recursive = TRUE, ignore.case = TRUE)

# lnt_read = read in a LexisNexis file
nu_data <- lnt_read(nu_data) 
```

```{r}
meta_df <- nu_data@meta
articles_df <- nu_data@articles
paragraphs_df <- nu_data@paragraphs
```

```{r}

headline_df<- data_frame(element_id = seq(1:length(meta_df$Headline)), 
                         Date = meta_df$Date, 
                         Headline = meta_df$Headline)

paragraphs_df <- data_frame(element_id = paragraphs_df$Art_ID, 
                            Text  = paragraphs_df$Paragraph)

ipm_data <- inner_join(headline_df, paragraphs_df, by = "element_id")
```

```{r}
my_files <- list.files(pattern = ".docx", path = here("data", "text_data"),
                       full.names = TRUE, recursive = TRUE, ignore.case = TRUE)
dat <- lnt_read(my_files) #Object of class 'LNT output'

meta_df <- dat@meta
articles_df <- dat@articles
paragraphs_df <- dat@paragraphs

dat2<- data_frame(element_id = seq(1:length(meta_df$Headline)), Date = meta_df$Date, Headline = meta_df$Headline)

paragraphs_dat <- data_frame(element_id = paragraphs_df$Art_ID, 
                             Text  = paragraphs_df$Paragraph)

# join the headlines with the paragraphs
dat3 <- inner_join(dat2, paragraphs_dat, by = "element_id") %>% clean_names()

```

```{r}
dat3 <- subset(dat3, text != " ")
dat3 <- dat3[!duplicated(dat3$text),]
dat3 <- dat3[!grepl("http", dat3$text),]
dat3 <- dat3[!grepl("Article Rating", dat3$text),]
dat3 <- dat3[!grepl("--IANS", dat3$text),]
dat3 <- dat3[!grepl("ZRL: Zero Risk Level", dat3$text),]
dat3 <- dat3[!grepl("ZOI: Zone Of Incorporation", dat3$text),]
dat3 <- dat3[!grepl("Contact", dat3$text),]
dat3 <- dat3[!grepl(" - ", dat3$text),]
dat3 <- dat3[!grepl("By", dat3$text),]
dat3 <- dat3[!grepl("Guest", dat3$text),]
dat3 <- dat3[!grepl("Share on", dat3$text),]
dat3 <- dat3[!grepl("Mar ", dat3$text),]
dat3 <- dat3[!grepl("Apr ", dat3$text),]
dat3 <- dat3[!grepl("zones", dat3$text),]
dat3 <- dat3[!grepl("Apr ", dat3$text),]
dat3 <- dat3[!grepl("Apr ", dat3$text),]
dat3 <- dat3[!grepl("Apr ", dat3$text),]
dat3 <- dat3[!grepl("Apr ", dat3$text),]

dat3 <- dat3 %>% 
  drop_na()
```

# Global FAO data on farming practices over time

Data was collected from the Food and Agriculture Organization (FAO) of the United Nations on global farming practices since 1960. The data was collected from 1990 to 2021 from the FAO data service website, and combined into one csv file. 


```{r Import Land Use Data}
farming_practices_full <- list.files(path = "data/farming_practices/", full.names = T)
farming_practices <- list.files(path = "data/farming_practices/", full.names = F)
practice_names <- str_replace(farming_practices, "fao_", "") %>% str_replace(".csv", "")
practices_df <- map(farming_practices_full, read_csv) %>% 
        map(~ select(., Area, Element, Item, Year, Unit, Value)) %>% 
        reduce(rbind)
```

```{r Summaries of Land Use}
summary_practices <- practices_df %>% 
        group_by(Item, Year, Unit) %>% 
        filter(Unit != "%") %>% 
        summarize(value = sum(Value))
```

```{r Land Use Graphs}
graph_practices <- function(topic) {
        
filtered_practice <- summary_practices %>% 
        filter(Item == topic)

filtered_practice %>% 
        ggplot(aes(x = Year, y = value)) + 
        geom_line() + 
        expand_limits(y = 0) +
        ylab(paste(filtered_practice$Unit[1], topic))
}

topics <- summary_practices %>%
        ungroup() %>% 
        select(Item) %>% 
        distinct() %>% 
        pull()

practices_graphs <- map(topics, graph_practices)
```

```{r Export Practices Graphs}
filenames_practices_graphs <- paste0(topics, ".pdf")
ggsave_med <- partial(ggsave, device = "pdf", width = 10, height = 6, units = "in")
map2(filenames_practices_graphs, practices_graphs, ggsave_med)
```

Let's adjust the summary we made earlier by adding a country grouping
```{r Summaries of Land Use by Country}
summary_practices <- practices_df %>% 
        group_by(Item, Year, Unit, Area) %>% 
        filter(Unit != "%") %>% 
        summarize(value = sum(Value))
```

Now, let's filter our data before we graph to only include the U.S.
```{r Just the U.S.}
graph_practices <- function(topic, country) {
        
filtered_practice <- summary_practices %>% 
        filter(Item == topic, Area == country)

filtered_practice %>% 
        ggplot(aes(x = Year, y = value)) + 
        geom_line() + 
        expand_limits(y = 0) +
        ylab(paste(filtered_practice$Unit[1], topic))
}

topics <- summary_practices %>%
        ungroup() %>% 
        select(Item) %>% 
        distinct() %>% 
        pull()

practices_graphs <- map2(topics, "United States of America", graph_practices)
```

```{r Export Just the U.S.}
filenames_practices_graphs <- paste0(topics, "us_only", ".pdf")
ggsave_med <- partial(ggsave, device = "pdf", width = 10, height = 6, units = "in")
map2(filenames_practices_graphs, practices_graphs, ggsave_med)
```

# Number of Articles over time.

Here we are looking at the number of articles that were published from 1990-2021. These articles were found using the Nexis Uni Database using the search terms "Integrated Pest Management" and "Sustainable farming".

```{r}
# Create a table that contains the total number of published articles for each year.
articles_by_year <- data.frame(year = 1990:2021, n_articles = 1)

# Add number of publications for each year.
articles_by_year[1,2] <- 9
articles_by_year[2,2] <- 10
articles_by_year[3,2] <- 7
articles_by_year[4,2] <- 36
articles_by_year[5,2] <- 57
articles_by_year[6,2] <- 76
articles_by_year[7,2] <- 89
articles_by_year[8,2] <- 76
articles_by_year[9,2] <- 67
articles_by_year[10,2] <- 70
articles_by_year[11,2] <- 52
articles_by_year[12,2] <- 47
articles_by_year[13,2] <- 56
articles_by_year[14,2] <- 69
articles_by_year[15,2] <- 70
articles_by_year[16,2] <- 78
articles_by_year[17,2] <- 82
articles_by_year[18,2] <- 100
articles_by_year[19,2] <- 176
articles_by_year[20,2] <- 207
articles_by_year[21,2] <- 227
articles_by_year[22,2] <- 234
articles_by_year[23,2] <- 386
articles_by_year[24,2] <- 381
articles_by_year[25,2] <- 411
articles_by_year[26,2] <- 485
articles_by_year[27,2] <- 696
articles_by_year[28,2] <- 606
articles_by_year[29,2] <- 710
articles_by_year[30,2] <- 905
articles_by_year[31,2] <- 1000
articles_by_year[32,2] <- 1118

```

```{r}
# Now plot the above data.
ggplot(articles_by_year, aes(x = year, y = n_articles)) +
  geom_line(col = "darkgreen") +
  labs(title = "Number of Articles Published by Year",
       subtitle = 'Search terms "Integrated pest management" and "Sustainable farming".',
       x = "Year",
       y = "Number of Articles Published")
```




